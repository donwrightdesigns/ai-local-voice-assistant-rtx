# Voice Assistant Configuration

# Ollama Configuration
ollama:
  base_url: "http://127.0.0.1:11434"
  model: "llama3.2:3b"
  parameters:
    temperature: 0.7
    top_p: 0.9
    top_k: 40

# Local Transformers (Phase 2)
local:
  # Set your local model paths here (Windows paths are fine)
  faster_path: "J:\\TOOLS\\voice-assistant-rtx\\voice-assistant-models\\mistral_7b_AWQ_int4_chat"
  advanced_path: "J:\\TOOLS\\voice-assistant-rtx\\voice-assistant-models\\llama2_13b_AWQ_INT4_chat"

# Text-to-Speech Configuration
tts:
  engine: "auto"  # Options: auto, kokoro, pyttsx3, windows, bark, none
  voice: "af_heart"  # Kokoro: af_heart, af_bella, af_sarah, am_adam, bf_emma, etc.
                     # Windows/pyttsx3: zira, david, etc.
                     # Bark: v2/en_speaker_6
  lang_code: "a"  # Kokoro language: 'a' (American), 'b' (British)

# Speech-to-Text Configuration
stt:
  model: "base.en"
  device: "cpu"  # Use "cuda" for GPU acceleration
  compute_type: "int8"  # Options: "float16", "int8", "int8_float16" for GPU; "int8" for CPU
  parameters:
    beam_size: 5

hotkeys:
  conversation: "ctrl+f2"
  dictation: "ctrl+f1"
  ai_typing: "f15"
  screen_ai: "f14"
  reset: "menu"
prompts:
  system_prompt: |
    You are a helpful and friendly AI assistant. You are polite, respectful, and aim to provide concise responses of less 
    than 20 words.
    The conversation transcript is as follows:
    {history}
    And here is the user's follow-up: {input}
    Your response:
