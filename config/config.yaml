# Voice Assistant Configuration

# Ollama Configuration
ollama:
  base_url: "http://127.0.0.1:11434"
  model: "llama3.2:3b"
  parameters:
    temperature: 0.9
    top_p: 0.9
    top_k: 40

# Local Transformers (Phase 2)
local:
  # Set your local model paths here (Windows paths are fine)
  faster_path: "J:\\TOOLS\\voice-assistant-rtx\\voice-assistant-models\\mistral_7b_AWQ_int4_chat"
  advanced_path: "J:\\TOOLS\\voice-assistant-rtx\\voice-assistant-models\\llama2_13b_AWQ_INT4_chat"

# Text-to-Speech Configuration
tts:
  engine: "auto"  # Options: auto, kokoro, pyttsx3, windows, bark, none
  voice: "af_heart"  # Kokoro: af_heart, af_bella, af_sarah, am_adam, bf_emma, etc.
                     # Windows/pyttsx3: zira, david, etc
  lang_code: "a"  # Kokoro language: 'a' (American), 'b' (British)

# Speech-to-Text Configuration
stt:
  model: "base.en"
  device: "cuda"  # Use "cuda" for GPU acceleration or "cpu" for basic processing
  compute_type: "int8"  # Options: "float16", "int8", "int8_float16" for GPU; "int8" for CPU
  parameters:
    beam_size: 9

hotkeys:
  conversation: "ctrl+f2"
  dictation: "ctrl+f1"
  ai_typing: "ctrl+f3"
  screen_ai: "ctrl+f4"
  reset: "menu"
prompts:
  system_prompt: |
    You are a fun but direct AI assistant. You inject humor and flirtation into 35% of your conversations.  You are always honest and direct, don't worry about hurting feelings.  You consider multiple  possibilities and try to keep responses less than 25 words.
    The conversation transcript is as follows:
    {history}
    And here is the user's follow-up: {input}
    Your response:
